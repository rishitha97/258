{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "258 Hw5 TFX_keras.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "wdeKOEkv1Fe8"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GivNBNYjb3b"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4SQA7Q5nej3"
      },
      "source": [
        "!pip install tfx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIqpWK9efviJ"
      },
      "source": [
        "#import required libraries\n",
        "import os\n",
        "import pprint\n",
        "import tempfile\n",
        "import urllib\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import absl\n",
        "import tensorflow as tf\n",
        "import tensorflow_model_analysis as tfma\n",
        "tf.get_logger().propagate = False\n",
        "pp = pprint.PrettyPrinter()\n",
        "\n",
        "import tfx\n",
        "from tfx.components import CsvExampleGen\n",
        "from tfx.utils.dsl_utils import external_input\n",
        "from tfx.components import Evaluator\n",
        "from tfx.components import ExampleValidator\n",
        "from tfx.components import Pusher\n",
        "from tfx.components import ResolverNode\n",
        "from tfx.components import SchemaGen\n",
        "from tfx.components import StatisticsGen\n",
        "from tfx.components import Trainer\n",
        "from tfx.components import Transform\n",
        "from tfx.components.base import executor_spec\n",
        "from tfx.components.trainer.executor import GenericExecutor\n",
        "from tfx.dsl.experimental import latest_blessed_model_resolver\n",
        "from tfx.orchestration import metadata\n",
        "from tfx.orchestration import pipeline\n",
        "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
        "from tfx.proto import pusher_pb2\n",
        "from tfx.proto import trainer_pb2\n",
        "from tfx.types import Channel\n",
        "from tfx.types.standard_artifacts import Model\n",
        "from tfx.types.standard_artifacts import ModelBlessing\n",
        "\n",
        "\n",
        "%load_ext tfx.orchestration.experimental.interactive.notebook_extensions.skip\n",
        "%reload_ext tfx.orchestration.experimental.interactive.notebook_extensions.skip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZ4K18_DN2D8"
      },
      "source": [
        "#check library versions\n",
        "print('TensorFlow version: {}'.format(tf.__version__))\n",
        "print('TFX version: {}'.format(tfx.__version__))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-a1A5oVaRRW"
      },
      "source": [
        "!rm -rf data.*\n",
        "!rm -rf *trainer.py\n",
        "!sudo rm -r /content/tfx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VaKBIxFaTCX"
      },
      "source": [
        "! mkdir /content/tfx/\n",
        "! mkdir /content/tfx/data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kITZRfb3bJ7o"
      },
      "source": [
        "## load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCDgE9UCbUq7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR9XXoIKbKg_"
      },
      "source": [
        "df=pd.read_csv('/content/drive/MyDrive/temp.csv',index_col=0)\n",
        "\n",
        "\n",
        "#Drop NA rows\n",
        "df = df.dropna()\n",
        "#df=df.drop(columns=['key'])\n",
        "\n",
        "##Keep a test set for final testing( TFX internally splits train and validation data )\n",
        "np.random.seed(seed=2)\n",
        "msk = np.random.rand(len(df)) < 0.9\n",
        "traindf = df[msk]\n",
        "evaldf = df[~msk]\n",
        "\n",
        "print(len(traindf))\n",
        "print(len(evaldf))\n",
        "\n",
        "traindf.to_csv(\"/content/tfx/data/data_trans.csv\", index=False, header=True)\n",
        "evaldf.to_csv(\"eval.csv\", index=False, header=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDhNnVBQbfwN"
      },
      "source": [
        "## Create interactive context"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zvh2GrIgbJUE"
      },
      "source": [
        "_tfx_root = os.path.join(os.getcwd(), 'tfx');\n",
        "context = InteractiveContext(pipeline_root=_tfx_root)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSY64AbFbnnu"
      },
      "source": [
        "## ExampleGen for Splitting of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtN1ATQxb4ye"
      },
      "source": [
        "example_gen = CsvExampleGen(input=external_input(os.path.join(_tfx_root, 'data')))\n",
        "context.run(example_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QE4eW7qcbnIW"
      },
      "source": [
        "#We get train ane evaulate splits of data\n",
        "artifact = example_gen.outputs['examples'].get()[0]\n",
        "print(artifact.split_names, artifact.uri)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6S3Q1cAb7XY"
      },
      "source": [
        "train_uri = os.path.join(example_gen.outputs['examples'].get()[0].uri, 'Split-train')\n",
        "\n",
        "tfrecord_filenames = [os.path.join(train_uri, name)\n",
        "                      for name in os.listdir(train_uri)]\n",
        "\n",
        "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
        "\n",
        "for tfrecord in dataset.take(3):\n",
        "  serialized_example = tfrecord.numpy()\n",
        "  example = tf.train.Example()\n",
        "  example.ParseFromString(serialized_example)\n",
        "  pp.pprint(example)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73H4FsmBcDoU"
      },
      "source": [
        "## StatisticsGen to compute statistics analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYlMXO4JcEWQ"
      },
      "source": [
        "statistics_gen = StatisticsGen(\n",
        "    examples=example_gen.outputs['examples'])\n",
        "context.run(statistics_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwkYcLshcH9Z"
      },
      "source": [
        "context.show(statistics_gen.outputs['statistics'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwyFMdXvcM-a"
      },
      "source": [
        "## SchemaGen to produce schema"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfhUfMt6cNoN"
      },
      "source": [
        "schema_gen = SchemaGen(\n",
        "    statistics=statistics_gen.outputs['statistics'],\n",
        "    infer_feature_shape=False)\n",
        "context.run(schema_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxX1iaxycY7R"
      },
      "source": [
        "context.show(schema_gen.outputs['schema'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCDjmVVRccDe"
      },
      "source": [
        "## Example valiator to detect anomalies in data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28olvpMNccpz"
      },
      "source": [
        "example_validator = ExampleValidator(\n",
        "    statistics=statistics_gen.outputs['statistics'],\n",
        "    schema=schema_gen.outputs['schema'])\n",
        "context.run(example_validator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iR0n-bybcjV9"
      },
      "source": [
        "context.show(example_validator.outputs['anomalies'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UaE3DNbclXy"
      },
      "source": [
        "# Get the URI of the output artifact representing the transformed examples, which is a directory\n",
        "train_uri = os.path.join(example_gen.outputs['examples'].get()[0].uri, 'Split-train')\n",
        "\n",
        "# Get the list of files in this directory (all compressed TFRecord files)\n",
        "tfrecord_filenames = [os.path.join(train_uri, name)\n",
        "                      for name in os.listdir(train_uri)]\n",
        "\n",
        "# Create a `TFRecordDataset` to read these files\n",
        "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
        "\n",
        "# Iterate over the first 1 records and decode them.\n",
        "for tfrecord in dataset.take(1):\n",
        "  serialized_example = tfrecord.numpy()\n",
        "  example = tf.train.Example()\n",
        "  example.ParseFromString(serialized_example)\n",
        "  pp.pprint(example)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GzuCpGscpNG"
      },
      "source": [
        "## Transform for feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezm8Lj79corq"
      },
      "source": [
        "bins_lat = pd.qcut(list(df['dropoff_latitude'].values) + list(df['pickup_latitude'].values), q=20, duplicates='drop', retbins=True)[1]\n",
        "bins_lon = pd.qcut(list(df['dropoff_longitude'].values) + list(df['pickup_longitude'].values), q=20, duplicates='drop', retbins=True)[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S63potXcufv"
      },
      "source": [
        "code = '''\n",
        "bins_lat = {bins_lat}\n",
        "bins_lon = {bins_lon}\n",
        "'''\n",
        "\n",
        "code = code.replace('{bins_lat}', str(list(bins_lat)))\n",
        "code = code.replace('{bins_lon}', str(list(bins_lon)))\n",
        "\n",
        "with open('constants_trainer.py', 'w') as writefile:\n",
        "    writefile.write(code)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7hvvbIVczzh"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnKQyIu5c0yL"
      },
      "source": [
        "_input_fn_module_file = 'inputfn_trainer.py'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPjhXuIF4YJh"
      },
      "source": [
        "%%writefile {_input_fn_module_file}\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "###############################\n",
        "##Feature engineering functions\n",
        "def feature_engg_features(df):\n",
        "  #Add new features\n",
        "  df['distance'] = ((df['pickup_latitude'] - df['dropoff_latitude'])**2 +  (df['pickup_longitude'] - df['dropoff_longitude'])**2)**0.5\n",
        "  \n",
        "  df['pickup_datetime'] = tf.strings.as_string(df['pickup_datetime'])\n",
        "\n",
        "  df['split_date_time'] = tf.strings.split(df['pickup_datetime'], ' ').to_tensor()\n",
        "\n",
        "  df['date'] = tf.gather(df['split_date_time'] ,0 , axis=1)\n",
        "  df['time'] = tf.gather(df['split_date_time'] ,1 , axis=1)\n",
        "\n",
        "  df['split_date'] = tf.strings.split(df['date'], '-',name='split_date').to_tensor()\n",
        "  df['split_time'] = tf.strings.split(df['time'], ':',name='split_time').to_tensor()\n",
        "\n",
        "  df['trip_start_month'] = tf.gather(df['split_date'] ,1 , axis=1)\n",
        "  df['trip_start_day'] = tf.gather(df['split_date'] ,2 , axis=1)\n",
        "  df['trip_start_hour'] = tf.gather(df['split_time'] ,0 , axis=1)\n",
        "  return(df)\n",
        "\n",
        "\n",
        "#To be called from TF\n",
        "def feature_engg(features, label):\n",
        "  #Add new features\n",
        "  features = feature_engg_features(features)\n",
        "\n",
        "  return(features, label)\n",
        "\n",
        "def make_input_fn(dir_uri, mode, vnum_epochs = None, batch_size = 512):\n",
        "    def decode_tfr(serialized_example):\n",
        "      # 1. define a parser\n",
        "      features = tf.io.parse_example(\n",
        "        serialized_example,\n",
        "        # Defaults are not specified since both keys are required.\n",
        "        features={\n",
        "            'dropoff_latitude': tf.io.FixedLenFeature([], tf.float32),\n",
        "            'dropoff_longitude': tf.io.FixedLenFeature([], tf.float32),\n",
        "            'fare_amount': tf.io.FixedLenFeature([], tf.float32),\n",
        "            'pickup_latitude': tf.io.FixedLenFeature([], tf.float32, default_value = 0.0),\n",
        "            'pickup_longitude': tf.io.FixedLenFeature([], tf.float32, default_value = 0.0),\n",
        "            'pickup_datetime': tf.io.FixedLenFeature([], tf., default_value = '2014-06-30 11:26:37')\n",
        "        })\n",
        "      \n",
        "\n",
        "      return features, features['fare_amount']\n",
        "\n",
        "    def _input_fn(v_test=False):\n",
        "      # Get the list of files in this directory (all compressed TFRecord files)\n",
        "      tfrecord_filenames = tf.io.gfile.glob(dir_uri)\n",
        "\n",
        "      # Create a `TFRecordDataset` to read these files\n",
        "      dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
        "      print('k')\n",
        "      if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "        num_epochs = vnum_epochs # indefinitely\n",
        "      else:\n",
        "        num_epochs = 1 # end-of-input after this\n",
        "\n",
        "      dataset = dataset.batch(batch_size)\n",
        "      dataset = dataset.prefetch(buffer_size = batch_size)\n",
        "\n",
        "      #Convert TFRecord data to dict\n",
        "      dataset = dataset.map(decode_tfr)\n",
        "\n",
        "      #Feature engineering\n",
        "      dataset = dataset.map(feature_engg)\n",
        "\n",
        "      if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "          num_epochs = vnum_epochs # indefinitely\n",
        "          dataset = dataset.shuffle(buffer_size = batch_size)\n",
        "      else:\n",
        "          num_epochs = 1 # end-of-input after this\n",
        "\n",
        "      dataset = dataset.repeat(num_epochs)       \n",
        "      \n",
        "      if v_test == True:\n",
        "        print(next(dataset.__iter__()))\n",
        "        \n",
        "      return dataset\n",
        "    return _input_fn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoHUTU4Dc93x"
      },
      "source": [
        "import inputfn_trainer as ift\n",
        "eval_file = os.path.join(example_gen.outputs['examples'].get()[0].uri, 'eval/*')\n",
        "fn_d = ift.make_input_fn(dir_uri = eval_file,\n",
        "                    mode = tf.estimator.ModeKeys.EVAL,\n",
        "                    batch_size = 10)\n",
        "fn_d(v_test=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYmxxx9A4YJn"
      },
      "source": [
        "%%writefile model_trainer.py\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import inputfn_trainer as ift\n",
        "import constants_trainer as ct\n",
        "\n",
        "from tfx.components.trainer.fn_args_utils import FnArgs\n",
        "print(tf.__version__)\n",
        "\n",
        "device = \"gpu\"\n",
        "\n",
        "if device == \"tpu\":\n",
        "  resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "  tf.config.experimental_connect_to_cluster(resolver)\n",
        "  tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "  strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
        "else:\n",
        "  strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
        "\n",
        "#Create model\n",
        "params_default = {\n",
        "    'lr' : 0.001,\n",
        "    'beta_1' : 0.99,\n",
        "    'beta_2' : 0.999,\n",
        "    'epsilon' : 1e-08,\n",
        "    'decay' : 0.01,\n",
        "    'hidden_layers' : 1\n",
        "}\n",
        "\n",
        "\n",
        "def create_feature_cols():\n",
        "    #Keras format features\n",
        "    k_month = tf.keras.Input(name='trip_start_month', shape=(1,), dtype=tf.string)\n",
        "    k_hour  = tf.keras.Input(name='trip_start_hour', shape=(1,), dtype=tf.string)\n",
        "    k_day  = tf.keras.Input(name='trip_start_day', shape=(1,), dtype=tf.string)\n",
        "    k_picklat  = tf.keras.Input(name='pickup_latitude', shape=(1,), dtype=tf.float32)\n",
        "    k_picklon  = tf.keras.Input(name='pickup_longitude', shape=(1,), dtype=tf.float32)\n",
        "    k_droplat  = tf.keras.Input(name='dropoff_latitude', shape=(1,), dtype=tf.float32)\n",
        "    k_droplon  = tf.keras.Input(name='dropoff_longitude', shape=(1,), dtype=tf.float32)\n",
        "    k_distance  = tf.keras.Input(name='distance', shape=(1,), dtype=tf.float32)\n",
        "    keras_dict_input = {'trip_start_month': k_month, 'trip_start_hour': k_hour, 'trip_start_day' : k_day,\n",
        "                        'pickup_latitude': k_picklat, 'pickup_longitude': k_picklon,\n",
        "                        'dropoff_latitude': k_droplat, 'dropoff_longitude': k_droplon, 'distance' : k_distance\n",
        "                        }\n",
        "\n",
        "    return({'K' : keras_dict_input})\n",
        "\n",
        "def create_keras_model(feature_cols, bins_lat, bins_lon,  params = params_default):\n",
        "    METRICS = [\n",
        "            keras.metrics.RootMeanSquaredError(name='rmse')\n",
        "    ]\n",
        "\n",
        "    #Input layers\n",
        "    input_feats = []\n",
        "    for inp in feature_cols['K'].keys():\n",
        "      input_feats.append(feature_cols['K'][inp])\n",
        "\n",
        "    \n",
        "\n",
        "    ##Handle categorical attributes( One-hot encoding )\n",
        "    cat_day = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=['1','2','3','4','5','6','7'], mask_token=None)(feature_cols['K']['trip_start_day'])\n",
        "    cat_day = tf.keras.layers.experimental.preprocessing.CategoryEncoding(max_tokens=7)(cat_day)\n",
        "\n",
        "    cat_hour = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=['1','2','3','4','5','6','7','8'\n",
        "                                                                                      '9','10','11','12','13','14','15','16',\n",
        "                                                                                      '17','18','19','20','21','22','23','0'\n",
        "                                                                                      ], mask_token=None)(feature_cols['K']['trip_start_hour'])\n",
        "    cat_hour = tf.keras.layers.experimental.preprocessing.CategoryEncoding(max_tokens=24)(cat_hour)\n",
        "\n",
        "    cat_month = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=['1','2','3','4','5','6','7','8'\n",
        "                                                                                      '9','10','11','12'], mask_token=None)(feature_cols['K']['trip_start_month'])\n",
        "    cat_month = tf.keras.layers.experimental.preprocessing.CategoryEncoding(max_tokens=12)(cat_month)\n",
        "\n",
        "    # cat_company = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=df['company'].unique(), mask_token=None)(feature_cols['K']['company'])\n",
        "    # cat_company = tf.keras.layers.experimental.preprocessing.CategoryEncoding(max_tokens=len(df['company'].unique()))(cat_company)\n",
        "\n",
        "    ##Binning\n",
        "    bins_pickup_lat = tf.keras.layers.experimental.preprocessing.Discretization(bins = bins_lat)(feature_cols['K']['pickup_latitude'])\n",
        "    cat_pickup_lat = tf.keras.layers.experimental.preprocessing.CategoryEncoding(len(bins_lat)+1)(bins_pickup_lat)\n",
        "\n",
        "    bins_pickup_lon = tf.keras.layers.experimental.preprocessing.Discretization(bins = bins_lon)(feature_cols['K']['pickup_longitude'])\n",
        "    cat_pickup_lon = tf.keras.layers.experimental.preprocessing.CategoryEncoding(len(bins_lon)+1)(bins_pickup_lon)\n",
        "\n",
        "    bins_drop_lat = tf.keras.layers.experimental.preprocessing.Discretization(bins = bins_lat)(feature_cols['K']['dropoff_latitude'])\n",
        "    cat_drop_lat = tf.keras.layers.experimental.preprocessing.CategoryEncoding(len(bins_lat)+1)(bins_drop_lat)\n",
        "\n",
        "    bins_drop_lon = tf.keras.layers.experimental.preprocessing.Discretization(bins = bins_lon)(feature_cols['K']['dropoff_longitude'])\n",
        "    cat_drop_lon = tf.keras.layers.experimental.preprocessing.CategoryEncoding(len(bins_lon)+1)(bins_drop_lon)\n",
        "\n",
        "    ##Categorical cross\n",
        "    cross_day_hour = tf.keras.layers.experimental.preprocessing.CategoryCrossing()([cat_day, cat_hour])\n",
        "    hash_cross_day_hour = tf.keras.layers.experimental.preprocessing.Hashing(num_bins=24 * 7)(cross_day_hour)\n",
        "    cat_cross_day_hour = tf.keras.layers.experimental.preprocessing.CategoryEncoding(max_tokens = 24* 7)(hash_cross_day_hour)\n",
        "\n",
        "    cross_pick_lon_lat = tf.keras.layers.experimental.preprocessing.CategoryCrossing()([cat_pickup_lat, cat_pickup_lon])\n",
        "    hash_cross_pick_lon_lat = tf.keras.layers.experimental.preprocessing.Hashing(num_bins=(len(bins_lat) + 1) ** 2)(cross_pick_lon_lat)\n",
        "\n",
        "    cross_drop_lon_lat = tf.keras.layers.experimental.preprocessing.CategoryCrossing()([cat_drop_lat, cat_drop_lon])\n",
        "    hash_cross_drop_lon_lat = tf.keras.layers.experimental.preprocessing.Hashing(num_bins=(len(bins_lat) + 1) ** 2)(cross_drop_lon_lat)\n",
        "\n",
        "    # Cross to embedding\n",
        "    embed_cross_pick_lon_lat = tf.keras.layers.Embedding(((len(bins_lat) + 1) ** 2), 4)(hash_cross_pick_lon_lat)\n",
        "    embed_cross_pick_lon_lat = tf.reduce_sum(embed_cross_pick_lon_lat, axis=-2)\n",
        "\n",
        "    embed_cross_drop_lon_lat = tf.keras.layers.Embedding(((len(bins_lat) + 1) ** 2), 4)(hash_cross_drop_lon_lat)\n",
        "    embed_cross_drop_lon_lat = tf.reduce_sum(embed_cross_drop_lon_lat, axis=-2)\n",
        "\n",
        "    # Also pass time attributes as Deep signal( Cast to integer )\n",
        "    int_trip_start_day = tf.strings.to_number(feature_cols['K']['trip_start_day'], tf.float32)\n",
        "    int_trip_start_hour = tf.strings.to_number(feature_cols['K']['trip_start_hour'], tf.float32)\n",
        "    int_trip_start_month = tf.strings.to_number(feature_cols['K']['trip_start_month'], tf.float32)\n",
        "\n",
        "    #Add feature engineered columns - LAMBDA layer\n",
        "\n",
        "    ###Create MODEL\n",
        "    ####Concatenate all features( Numerical input )\n",
        "    x_input_numeric = tf.keras.layers.concatenate([\n",
        "                    feature_cols['K']['pickup_latitude'], feature_cols['K']['pickup_longitude'],\n",
        "                    feature_cols['K']['dropoff_latitude'], feature_cols['K']['dropoff_longitude'],\n",
        "                    feature_cols['K']['distance'], embed_cross_pick_lon_lat, embed_cross_drop_lon_lat,\n",
        "                    int_trip_start_day, int_trip_start_hour, int_trip_start_month\n",
        "                    ])\n",
        "\n",
        "    #DEEP - This Dense layer connects to input layer - Numeric Data\n",
        "    x_numeric = tf.keras.layers.Dense(32, activation='relu', kernel_initializer=\"he_uniform\")(x_input_numeric)\n",
        "    x_numeric = tf.keras.layers.BatchNormalization()(x_numeric)\n",
        "\n",
        "    ####Concatenate all Categorical features( Categorical converted )\n",
        "    x_input_categ = tf.keras.layers.concatenate([\n",
        "                    cat_month, cat_cross_day_hour, cat_pickup_lat, cat_pickup_lon,\n",
        "                    cat_drop_lat, cat_drop_lon\n",
        "                    ])\n",
        "    \n",
        "    #WIDE - This Dense layer connects to input layer - Categorical Data\n",
        "    x_categ = tf.keras.layers.Dense(32, activation='relu', kernel_initializer=\"he_uniform\")(x_input_categ)\n",
        "\n",
        "    ####Concatenate both Wide and Deep layers\n",
        "    x = tf.keras.layers.concatenate([x_categ, x_numeric])\n",
        "\n",
        "    for l_ in range(params['hidden_layers']):\n",
        "        x = tf.keras.layers.Dense(32, activation='relu', kernel_initializer=\"he_uniform\",\n",
        "                                  activity_regularizer=tf.keras.regularizers.l2(0.00001))(x)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    #Final Layer\n",
        "    out = tf.keras.layers.Dense(1, activation='relu')(x)\n",
        "    model = tf.keras.Model(input_feats, out)\n",
        "\n",
        "    #Set optimizer\n",
        "    opt = tf.keras.optimizers.Adam(lr= params['lr'], beta_1=params['beta_1'], \n",
        "                                        beta_2=params['beta_2'], epsilon=params['epsilon'])\n",
        "\n",
        "    #Compile model\n",
        "    model.compile(loss='mean_squared_error',  optimizer=opt, metrics = METRICS)\n",
        "\n",
        "    #Print Summary\n",
        "    print(model.summary())\n",
        "    return model\n",
        "\n",
        "def keras_train_and_evaluate(model, train_dataset, validation_dataset, epochs=100):\n",
        "  #Add callbacks\n",
        "  reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                                patience=5, min_lr=0.00001, verbose = 1)\n",
        "  \n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\")\n",
        "\n",
        "  #Train and Evaluate\n",
        "  out = model.fit(train_dataset, \n",
        "                  validation_data = validation_dataset,\n",
        "                  epochs=epochs,\n",
        "                  # validation_steps = 3,   \n",
        "                  steps_per_epoch = 100,   \n",
        "                  callbacks=[reduce_lr,  \n",
        "                             keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True, verbose=True)]\n",
        "                  )\n",
        "  return model\n",
        "\n",
        "def save_model(model, model_save_path):\n",
        "  @tf.function\n",
        "  def serving(dropoff_latitude, dropoff_longitude, pickup_latitude, pickup_longitude, trip_start_day, trip_start_hour, trip_start_month):\n",
        "      distance = tf.cast( tf.sqrt((tf.abs(dropoff_latitude - pickup_latitude))**2 + (tf.abs(dropoff_longitude - pickup_longitude))**2), tf.float32)\n",
        "\n",
        "      payload = {\n",
        "          'dropoff_latitude': dropoff_latitude,\n",
        "          'dropoff_longitude': dropoff_longitude,\n",
        "          'pickup_latitude': pickup_latitude,\n",
        "          'pickup_longitude': pickup_longitude,\n",
        "          'trip_start_day': trip_start_day,\n",
        "          'trip_start_hour': trip_start_hour,\n",
        "          'trip_start_month': trip_start_month,\n",
        "          'distance': distance\n",
        "      }\n",
        "      \n",
        "      predictions = model(payload)\n",
        "      return predictions\n",
        "\n",
        "  serving = serving.get_concrete_function(trip_start_day=tf.TensorSpec([None,], dtype= tf.string, name='trip_start_day'), \n",
        "                                          trip_start_hour=tf.TensorSpec([None,], dtype= tf.string, name='trip_start_hour'),\n",
        "                                          trip_start_month=tf.TensorSpec([None], dtype= tf.string, name='trip_start_month'), \n",
        "                                          dropoff_latitude=tf.TensorSpec([None,], dtype= tf.float32, name='dropoff_latitude'),\n",
        "                                          dropoff_longitude=tf.TensorSpec([None,], dtype= tf.float32, name='dropoff_longitude'), \n",
        "                                          pickup_latitude=tf.TensorSpec([None,], dtype= tf.float32, name='pickup_latitude'),\n",
        "                                          pickup_longitude=tf.TensorSpec([None,], dtype= tf.float32, name='pickup_longitude')\n",
        "                                          )\n",
        "\n",
        "  # version = \"1\"  #{'serving_default': call_output}\n",
        "  tf.saved_model.save(\n",
        "      model,\n",
        "      model_save_path + \"/\",\n",
        "      signatures=serving\n",
        "  )\n",
        "\n",
        "##Main function called by TFX\n",
        "def run_fn(fn_args: FnArgs):\n",
        "  train_dataset = ift.make_input_fn(dir_uri = fn_args.train_files,\n",
        "                      mode = tf.estimator.ModeKeys.TRAIN,\n",
        "                      batch_size = 128)()\n",
        "\n",
        "  validation_dataset = ift.make_input_fn(dir_uri = fn_args.eval_files,\n",
        "                      mode = tf.estimator.ModeKeys.EVAL,\n",
        "                      batch_size = 512)()\n",
        "\n",
        "  m_ = create_keras_model(params = params_default, feature_cols = create_feature_cols(),\n",
        "                          bins_lat = ct.bins_lat,\n",
        "                          bins_lon = ct.bins_lon)\n",
        "  tf.keras.utils.plot_model(m_, show_shapes=True, rankdir=\"LR\")\n",
        "\n",
        "  m_ = keras_train_and_evaluate(m_, train_dataset, validation_dataset, fn_args.custom_config['epochs'])\n",
        "\n",
        "  save_model(m_, fn_args.serving_model_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "429-vvCWibO0"
      },
      "source": [
        "_model_trainer_module_file='model_trainer.py'\n",
        "trainer = Trainer(\n",
        "    module_file=os.path.abspath(_model_trainer_module_file),\n",
        "    custom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor),\n",
        "    examples=example_gen.outputs['examples'],\n",
        "    train_args=trainer_pb2.TrainArgs(),\n",
        "    eval_args=trainer_pb2.EvalArgs(),\n",
        "    custom_config=({\"epochs\": 1})\n",
        "    )\n",
        "\n",
        "context.run(trainer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELRG3CgvdeMa"
      },
      "source": [
        "## Analyzing pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPppkNbBdh6j"
      },
      "source": [
        "model_artifact_dir = trainer.outputs['model'].get()[0].uri\n",
        "pp.pprint(os.listdir(model_artifact_dir))\n",
        "model_dir = os.path.join(model_artifact_dir, 'serving_model_dir')\n",
        "pp.pprint(os.listdir(model_dir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8DYekCZlHfj"
      },
      "source": [
        "## Pusher to verify validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07-KMjUzdpBa"
      },
      "source": [
        "pusher = Pusher(\n",
        "    model=trainer.outputs['model'],\n",
        "    push_destination=pusher_pb2.PushDestination(\n",
        "        filesystem=pusher_pb2.PushDestination.Filesystem(\n",
        "            base_directory=_serving_model_dir)))\n",
        "context.run(pusher)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}